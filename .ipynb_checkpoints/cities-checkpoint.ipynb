{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topos Data Engineer Intern Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "# from shapely.geometry import Polygon\n",
    "from fiona.crs import from_epsg\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting soup from Wikipedia on top US cities\n",
    "def soup_data():\n",
    "    # Get the soup from Wikipedia\n",
    "    cities_soup = BeautifulSoup(requests.get(\"https://en.wikipedia.org/wiki/\" + \\\n",
    "                                 \"List_of_United_States_cities_by_population\").content, \"html.parser\")\n",
    "    # Get all the data for all cities\n",
    "    cities_data = cities_soup.findAll('table', attrs={'class':'wikitable sortable'})[0].findAll('tr')[1:]\n",
    "    return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all the basic info on each city\n",
    "def basic_info():\n",
    "    # Create a dictionary to store everything\n",
    "    cities_dict = {}\n",
    "    \n",
    "    # Get the soup data\n",
    "    cities_data = soup_data()\n",
    "    \n",
    "    # Iterate through each city\n",
    "    for city in cities_data:\n",
    "        \n",
    "        # Create a dictionary for each city's individual data\n",
    "        city_dict = {'name':None,'state':None,'st_cap':0,'st_large':0,'st_cap_large':0}\n",
    "        \n",
    "        # Get all the data\n",
    "        all_info = city.findAll('td')\n",
    "        \n",
    "        # Check for DC\n",
    "        if all_info[1].text.split('[')[0].strip() == 'Washington, D.C.':\n",
    "            city_dict['fed_cap'] = 1\n",
    "        else:\n",
    "            city_dict['fed_cap'] = 0\n",
    "        \n",
    "        # Rank, city name, and state\n",
    "        rank = int(all_info[0].text.strip())\n",
    "        city_dict.update({'name':all_info[1].text.split('[')[0].strip()})\n",
    "        city_dict.update({'state':all_info[2].text.strip()})\n",
    "        \n",
    "        # To get capital/largest city data\n",
    "        style = str(all_info[1])\n",
    "        if 'ffff99' in style:\n",
    "            city_dict.update({'st_cap':1})\n",
    "        elif 'cfecec' in style: \n",
    "            city_dict.update({'st_large':1})\n",
    "        elif 'ccff99' in style:\n",
    "            city_dict.update({'st_cap_large':1})\n",
    "        \n",
    "        # Get the rest of the data\n",
    "        city_dict['pop'] = all_info[3].text.replace(',','').strip()\n",
    "        city_dict['cen'] = all_info[4].text.replace(',','').strip()\n",
    "        city_dict['area_mi'] = all_info[6].text.split()[0].replace(',','').strip()\n",
    "        city_dict['area_km'] = all_info[7].text.split()[0].replace(',','').strip()\n",
    "        city_dict['pd_mi'] = all_info[8].text.split('/')[0].replace(',','').strip()\n",
    "        city_dict['pd_km'] = all_info[9].text.split('/')[0].replace(',','').strip()\n",
    "        \n",
    "        # Get the location\n",
    "        loc_data = all_info[10].text.split('/')[2].strip().split(';')\n",
    "        city_dict['lon'] = loc_data[1].split(\"\\ufeff\")[0].strip()\n",
    "        city_dict['lat'] = loc_data[0]\n",
    "\n",
    "        # To handle population change data\n",
    "        change = all_info[5].text.strip().split('%')[0]\n",
    "        if '+' in change:\n",
    "            city_dict['change'] = change.split('+')[1]\n",
    "        elif '−' in change:\n",
    "            city_dict['change'] = '-' + change.split('−')[1]\n",
    "        else:\n",
    "            city_dict['change'] = 0 # this is for only one outlier case from Jurupa Valley, CA\n",
    "    \n",
    "        # Create the dict of dicts\n",
    "        cities_dict[rank] = city_dict\n",
    "    \n",
    "    # Return the data to be fed into a dataframe\n",
    "    return cities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more data from each city\n",
    "def more_data():\n",
    "    # Create a dictionary to store everything\n",
    "    cities_dict = {}\n",
    "    \n",
    "    # Get the soup\n",
    "    cities_data = soup_data()\n",
    "    \n",
    "    # Iterate through each city\n",
    "    for city in cities_data:\n",
    "                \n",
    "        # Create a dictionary for each city's individual data\n",
    "        city_dict = {'mayor_party':None,'elevation':None,'tz':None,'demo':None,'city_site':None}\n",
    "        \n",
    "        # Get the info for each city in the master table\n",
    "        all_info = city.findAll('td')\n",
    "        \n",
    "        # Get the rank for dataframe merging later on\n",
    "        rank = int(all_info[0].text.strip())\n",
    "\n",
    "        # Get that city's wikipedia page URL\n",
    "        wiki_site = 'https://en.wikipedia.org' + all_info[1].find('a').get('href')\n",
    "        city_dict['wiki_site'] = wiki_site\n",
    "        \n",
    "        # Make a new soup out of that URL\n",
    "        city_soup = BeautifulSoup(requests.get(wiki_site).content, \"html.parser\")\n",
    "        \n",
    "        # Go through the info summary at the top right of each page\n",
    "        info_sum = city_soup.find('table', attrs={'class':'infobox geography vcard'}).tbody\n",
    "        \n",
    "        # Iterate\n",
    "        for section in info_sum:\n",
    "            if section.th:\n",
    "                # Get the governing party for each mayor\n",
    "                if ('Mayor' in section.th.text) and (len(section.td.findAll('a'))>1):\n",
    "                    if any(i.isdigit() for i in section.td.findAll('a')[1].text) == True:\n",
    "                        continue\n",
    "                    else:\n",
    "                        city_dict.update({'mayor_party':section.td.findAll('a')[1].text\\\n",
    "                                          .replace(\"(\",\"\").replace(\")\",\"\")})\n",
    "                # Get the elevation, time zone, and website\n",
    "                if 'Elevation' in section.th.text:\n",
    "                    city_dict.update({'elevation':section.td.text.split()[0].replace(',','')})\n",
    "                if 'Time zone' in section.th.text:\n",
    "                    city_dict.update({'tz':section.td.text.split()[0]})\n",
    "                if 'Website' in section.th.text:\n",
    "                    city_dict.update({'city_site':section.td.find('a').get('href')})\n",
    "                \n",
    "                # For demonym section specifically\n",
    "                section_str = str(section)\n",
    "                # Take the first demonym for each city\n",
    "                if 'Demonym' in section_str:\n",
    "                    if '<br/>' in section_str:\n",
    "                        split1 = section_str.split('<td>')[1].split('<br/>')[0]\n",
    "                        if '<sup' in split1:\n",
    "                            city_dict.update({'demo':split1.split('<sup')[0]})\n",
    "                        else:\n",
    "                            city_dict.update({'demo':section_str.split('<td>')[1].split('<br/>')[0]\\\n",
    "                                              .replace(',','').strip()})\n",
    "                    else:\n",
    "                        if '<li>' in section_str:\n",
    "                            city_dict.update({'demo':section.td.find('li').text})\n",
    "                        else:\n",
    "                            city_dict.update({'demo':section.td.text.split(',')[0].split('[')[0]\\\n",
    "                                              .replace(',','').strip()})\n",
    "                    \n",
    "        # Create the dict of dicts\n",
    "        cities_dict[rank] = city_dict\n",
    "            \n",
    "    return cities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = more_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame.from_dict(test, orient='index').reset_index().rename(columns={'index':'rank'})\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe from all the data\n",
    "def df_maker():\n",
    "    # Get the basic city info\n",
    "    basic_data = basic_info()\n",
    "    \n",
    "    # Get the extra info from each city's wikipedia page\n",
    "    extra_city_data = more_data()\n",
    "    \n",
    "    # Make dataframes\n",
    "    basic_df = pd.DataFrame.from_dict(basic_data, orient='index').reset_index().rename(columns={'index':'rank'})\\\n",
    "                .apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    extra_df = pd.DataFrame.from_dict(extra_city_data, orient='index').reset_index().rename(\\\n",
    "                columns={'index':'rank'})\n",
    "\n",
    "    # Merge them\n",
    "    merged_dfs = basic_df.merge(extra_df, how='left', left_on='rank', right_on='rank')\n",
    "    \n",
    "    # Create geometry column for geoplotting\n",
    "    merged_dfs['lonlat'] = list(zip(merged_dfs.lon, merged_dfs.lat))\n",
    "    merged_dfs['geometry'] = merged_dfs[['lonlat']].applymap(lambda x:shapely.geometry.Point(x))\n",
    "    merged_dfs = gpd.GeoDataFrame(merged_dfs)\n",
    "    merged_dfs.crs = from_epsg(4326)\n",
    "    merged_dfs = merged_dfs.to_crs(epsg=4269)\n",
    "\n",
    "    return merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "df = df_maker()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values after full data scrape in each column\n",
    "print(\"Columns with any missing data:\")\n",
    "print(\"\")\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum()>0:\n",
    "        print(\"Column name: {}\".format(col))\n",
    "        print(\"Number of missing values: {} out of {} rows\".format(df[col].isnull().sum(), df.shape[0]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export data (withough headers in order to mimic the steps provided by Google Cloud: \n",
    "https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui)\n",
    "\"\"\"\n",
    "df.to_csv('cities.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data for BigQuery\n",
    "\n",
    "Use the following query to reorder the data:\n",
    "\n",
    "SELECT * FROM `topos-242022.topos.topos` ORDER BY rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure data loads properly from saved file\n",
    "test_load = pd.read_csv('cities.txt', header=None)\n",
    "test_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names and data types to enter into BigQuery during table creation\n",
    "columns = df.dtypes.index.tolist()\n",
    "dtypes = df.dtypes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the data types so they align with Google's data type names\n",
    "for i in range(len(columns)):\n",
    "    if dtypes[i] == 'int64':\n",
    "        dtypes[i] = 'integer'\n",
    "    elif dtypes[i] == 'float64':\n",
    "        dtypes[i] = 'float'\n",
    "    elif dtypes[i] == 'object':\n",
    "        dtypes[i] = 'string'\n",
    "    print(str(columns[i]) + \":\" + str(dtypes[i]) + \",\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "\n",
    "With state boundary data available via the US Census TIGER/Line shapefiles, we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pulling in shapefile boundaries for the US (uncomment to rerun):\n",
    "# url = 'https://www2.census.gov/geo/tiger/TIGER2017/STATE/tl_2017_us_state.zip'\n",
    "# urllib.request.urlretrieve(url, 'tl_2017_us_state.zip')\n",
    "# !mkdir us_bound\n",
    "# !unzip tl_2017_us_state.zip -d us_bound\n",
    "# !rm -r tl_2017_us_state.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "us_shp = gpd.read_file('us_bound/tl_2017_us_state.shp')\n",
    "us_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting total pop\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('xkcd:sky blue')\n",
    "us_shp.plot(alpha=1,linewidth=0.8,ax=ax,color='g',edgecolor='black')\n",
    "ax.scatter(df.lon, df.lat, s=df.cen/5000, alpha=.5, color='orange')\n",
    "plt.title(\"Map of Population in US Cities\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "# Manually setting x and y limits since US territories and parts of Alaska otherwise throw off the plot\n",
    "ax.set_xlim(-181, -62)\n",
    "ax.set_ylim(15, 73)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting pop density\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('xkcd:sky blue')\n",
    "us_shp.plot(alpha=1,linewidth=0.8,ax=ax,color='g',edgecolor='black')\n",
    "ax.scatter(df.lon, df.lat, s=df.pd_mi/50, alpha=.5, color='orange')\n",
    "plt.title(\"Map of Population Density in US Cities\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.set_xlim(-181, -62)\n",
    "ax.set_ylim(15, 73)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting colors for the following two plots\n",
    "colors = {'D':'blue',None:'green','I':'orange','R':'red','DFL':'pink','ID':'purple'}\n",
    "\n",
    "# Create a patchlist for the legend\n",
    "color_names = {'Democrat':'blue','Not listed on WP':'green','Independent':'orange','Republican':'red',\\\n",
    "               'Minnesota Democratic–Farmer–Labor Party':'pink','Independent Democrats':'purple'}\n",
    "patchList = []\n",
    "for key in color_names:\n",
    "    data_key = mpatches.Patch(color=color_names[key], label=key)\n",
    "    patchList.append(data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mayor parties (no size scaling)\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('xkcd:sky blue')\n",
    "us_shp.plot(alpha=1,linewidth=0.8,ax=ax,color='w',edgecolor='black')\n",
    "ax.scatter(df.lon, df.lat, alpha=.5, s=100, c=df.mayor_party.apply(lambda x: colors[x]))\n",
    "plt.title(\"Mayor Parties in US Cities\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.set_xlim(-181, -62)\n",
    "ax.set_ylim(15, 73)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.legend(handles=patchList, loc='upper center', bbox_to_anchor=(0.5, -0.001),ncol=3,fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mayor parties (with size scaling)\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('xkcd:sky blue')\n",
    "us_shp.plot(alpha=1,linewidth=0.8,ax=ax,color='w',edgecolor='black')\n",
    "ax.scatter(df.lon, df.lat, alpha=.5, s=df.cen/5000, c=df.mayor_party.apply(lambda x: colors[x]))\n",
    "plt.title(\"Mayor Parties in US Cities by Population Size\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "ax.set_xlim(-181, -62)\n",
    "ax.set_ylim(15, 73)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.legend(handles=patchList, loc='upper center', bbox_to_anchor=(0.5, -0.001),ncol=3,fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "We can see that many of the larger US cities have Democratic mayors. Many cities, unfortunately, did not include an abbreviated party affiliation for the city's mayor in the summary tables of their respective Wikipedia pages and are shaded in green."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
